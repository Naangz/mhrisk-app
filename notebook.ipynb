{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "be019f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import skops.io as sio\n",
    "from evidently.presets import DataDriftPreset\n",
    "from evidently import Report\n",
    "from evidently import Dataset, DataDefinition\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "461ecfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Setup direktori yang diperlukan\"\"\"\n",
    "    directories = [\"models\", \"results\", \"monitoring\", \"monitoring/evidently_reports\"]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    print(\"✅ Directories setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c27d0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load dan prepare data dengan proper encoding untuk menghindari serialization issues\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    data_files = ['data/mental_health_lite.csv', 'data/mental_health_life_cut.csv']\n",
    "    df = None\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"✅ Dataset loaded from: {file_path}\")\n",
    "            break\n",
    "    \n",
    "    if df is None:\n",
    "        raise FileNotFoundError(\"No dataset found\")\n",
    "    \n",
    "    print(f\"📊 Original data shape: {df.shape}\")\n",
    "    print(f\"📊 Original data types:\\n{df.dtypes}\")\n",
    "    \n",
    "    # ✅ CRITICAL FIX: Proper categorical encoding\n",
    "    encoders = {}\n",
    "    categorical_cols = ['gender', 'employment_status', 'work_environment', \n",
    "                       'mental_health_history', 'seeks_treatment']\n",
    "    \n",
    "    # Encode categorical variables PROPERLY\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"🔧 Encoding {col}...\")\n",
    "            le = LabelEncoder()\n",
    "            \n",
    "            # Handle missing values first\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "            \n",
    "            # Encode to numeric\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "            encoders[col] = le\n",
    "            \n",
    "            print(f\"✅ {col} encoded: {le.classes_} -> {list(range(len(le.classes_)))}\")\n",
    "    \n",
    "    # Encode target variable\n",
    "    if 'mental_health_risk' in df.columns:\n",
    "        le_risk = LabelEncoder()\n",
    "        df['mental_health_risk'] = df['mental_health_risk'].fillna('Unknown')\n",
    "        df['risk_encoded'] = le_risk.fit_transform(df['mental_health_risk'].astype(str))\n",
    "        encoders['risk'] = le_risk\n",
    "        print(f\"✅ Target encoded: {le_risk.classes_} -> {list(range(len(le_risk.classes_)))}\")\n",
    "    \n",
    "    # ✅ CRITICAL: Remove original categorical columns to avoid confusion\n",
    "    columns_to_drop = [col for col in categorical_cols if col in df.columns]\n",
    "    columns_to_drop.append('mental_health_risk')  # Remove original target\n",
    "    \n",
    "    df_clean = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # ✅ Ensure all remaining columns are numeric\n",
    "    for col in df_clean.columns:\n",
    "        if col != 'risk_encoded':\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Fill any remaining NaN values\n",
    "    df_clean = df_clean.fillna(0)\n",
    "    \n",
    "    print(f\"📊 Cleaned data shape: {df_clean.shape}\")\n",
    "    print(f\"📊 Cleaned data types:\\n{df_clean.dtypes}\")\n",
    "    \n",
    "    # Verify all data is numeric\n",
    "    non_numeric_cols = df_clean.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric_cols:\n",
    "        print(f\"⚠️ Non-numeric columns found: {non_numeric_cols}\")\n",
    "        for col in non_numeric_cols:\n",
    "            if col != 'risk_encoded':\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Save encoders\n",
    "    os.makedirs(\"model\", exist_ok=True)\n",
    "    with open(\"model/encoders.pkl\", \"wb\") as f:\n",
    "        pickle.dump(encoders, f)\n",
    "    \n",
    "    # Save reference data for monitoring\n",
    "    reference_path = \"monitoring/reference_data.csv\"\n",
    "    os.makedirs(\"monitoring\", exist_ok=True)\n",
    "    df_clean.to_csv(reference_path, index=False)\n",
    "    print(\"✅ Reference data saved for future monitoring\")\n",
    "    \n",
    "    return df_clean, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dd44811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evidently_data_report(df, data_source):\n",
    "    \"\"\"Create Evidently data quality report\"\"\"\n",
    "    print(\"📊 Creating Evidently data quality report...\")\n",
    "    \n",
    "    try:\n",
    "        from evidently import Report, Dataset, DataDefinition\n",
    "        from evidently.presets import DataQualityPreset\n",
    "        import os\n",
    "        \n",
    "        # Buat direktori jika belum ada\n",
    "        os.makedirs(\"monitoring/evidently_reports\", exist_ok=True)\n",
    "        \n",
    "        # Identifikasi tipe kolom secara otomatis\n",
    "        numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Definisikan struktur data\n",
    "        data_definition = DataDefinition(\n",
    "            numerical_columns=numerical_columns,\n",
    "            categorical_columns=categorical_columns\n",
    "        )\n",
    "        \n",
    "        # Buat Dataset object\n",
    "        current_data = Dataset.from_pandas(\n",
    "            df,\n",
    "            data_definition=data_definition\n",
    "        )\n",
    "        \n",
    "        # Buat report dengan DataQualityPreset\n",
    "        data_report = Report([\n",
    "            DataQualityPreset()\n",
    "        ])\n",
    "        \n",
    "        # Jalankan report\n",
    "        data_report.run(current_data=current_data, reference_data=None)\n",
    "        \n",
    "        # Save HTML report\n",
    "        report_path = \"monitoring/evidently_reports/data_quality_report.html\"\n",
    "        data_report.save_html(report_path)\n",
    "        \n",
    "        # Extract key metrics dari report\n",
    "        report_dict = data_report.as_dict()\n",
    "        \n",
    "        # Hitung statistik dasar\n",
    "        missing_values_per_column = df.isnull().sum()\n",
    "        total_missing = missing_values_per_column.sum()\n",
    "        \n",
    "        data_quality_summary = {\n",
    "            \"report_generated\": True,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"numerical_columns\": len(numerical_columns),\n",
    "            \"categorical_columns\": len(categorical_columns),\n",
    "            \"missing_values_count\": int(total_missing),\n",
    "            \"missing_values_percentage\": round((total_missing / (len(df) * len(df.columns))) * 100, 2),\n",
    "            \"columns_with_missing\": missing_values_per_column[missing_values_per_column > 0].to_dict(),\n",
    "            \"report_path\": report_path,\n",
    "            \"data_definition\": {\n",
    "                \"numerical_columns\": numerical_columns,\n",
    "                \"categorical_columns\": categorical_columns\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(data_quality_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"✅ Evidently data quality report saved to {report_path}\")\n",
    "        print(f\"📈 Data summary: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        print(f\"🔢 Numerical columns: {len(numerical_columns)}\")\n",
    "        print(f\"📝 Categorical columns: {len(categorical_columns)}\")\n",
    "        print(f\"❌ Missing values: {total_missing} ({data_quality_summary['missing_values_percentage']}%)\")\n",
    "        \n",
    "        return data_quality_summary\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"⚠️ Import error - Evidently version mismatch: {e}\")\n",
    "        print(\"💡 Tip: Pastikan menggunakan evidently>=0.7.0\")\n",
    "        \n",
    "        # Create fallback summary\n",
    "        fallback_summary = {\n",
    "            \"report_generated\": False,\n",
    "            \"error\": f\"Import error: {str(e)}\",\n",
    "            \"error_type\": \"import_error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"missing_values_count\": int(df.isnull().sum().sum())\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(fallback_summary, f, indent=2)\n",
    "            \n",
    "        return fallback_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Evidently data quality report failed: {e}\")\n",
    "        \n",
    "        # Create fallback summary\n",
    "        fallback_summary = {\n",
    "            \"report_generated\": False,\n",
    "            \"error\": str(e),\n",
    "            \"error_type\": \"runtime_error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"missing_values_count\": int(df.isnull().sum().sum())\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(fallback_summary, f, indent=2)\n",
    "            \n",
    "        return fallback_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e4297a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical features dengan proper handling\"\"\"\n",
    "    print(\"🔧 Encoding categorical features...\")\n",
    "    \n",
    "    encoders = {}\n",
    "    \n",
    "    # Identifikasi kolom kategorik\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Exclude target column jika ada\n",
    "    target_candidates = ['mental_health_condition', 'target', 'label', 'class']\n",
    "    target_column = None\n",
    "    \n",
    "    for col in target_candidates:\n",
    "        if col in df.columns:\n",
    "            target_column = col\n",
    "            if col in categorical_columns:\n",
    "                categorical_columns.remove(col)\n",
    "            break\n",
    "    \n",
    "    print(f\"🎯 Target column identified: {target_column}\")\n",
    "    print(f\"📝 Categorical columns to encode: {categorical_columns}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            encoder = LabelEncoder()\n",
    "            # Handle missing values\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            df[f\"{col}_encoded\"] = encoder.fit_transform(df[col])\n",
    "            encoders[col] = encoder\n",
    "            print(f\"✅ Encoded {col} -> {col}_encoded\")\n",
    "    \n",
    "    # Encode target column jika kategorik\n",
    "    if target_column and df[target_column].dtype == 'object':\n",
    "        target_encoder = LabelEncoder()\n",
    "        df[f\"{target_column}_encoded\"] = target_encoder.fit_transform(df[target_column])\n",
    "        encoders['target'] = target_encoder\n",
    "        target_column = f\"{target_column}_encoded\"\n",
    "        print(f\"✅ Encoded target column: {target_column}\")\n",
    "    \n",
    "    return df, encoders, target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "add95cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, target_column):\n",
    "    \"\"\"Prepare features untuk training\"\"\"\n",
    "    print(\"🎯 Preparing features for training...\")\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_columns = [\n",
    "        target_column,\n",
    "        'id', 'index', 'timestamp', 'date'\n",
    "    ]\n",
    "    \n",
    "    # Add original categorical columns to exclude\n",
    "    categorical_originals = [col for col in df.columns if col.endswith('_encoded')]\n",
    "    for encoded_col in categorical_originals:\n",
    "        original_col = encoded_col.replace('_encoded', '')\n",
    "        if original_col in df.columns:\n",
    "            exclude_columns.append(original_col)\n",
    "    \n",
    "    # Select feature columns\n",
    "    feature_columns = []\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_columns and df[col].dtype in ['int64', 'float64']:\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    print(f\"📊 Selected features: {feature_columns}\")\n",
    "    print(f\"🎯 Target column: {target_column}\")\n",
    "    \n",
    "    if len(feature_columns) == 0:\n",
    "        raise ValueError(\"❌ No valid numeric features found for training\")\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "590a17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(df, encoders):\n",
    "    \"\"\"Prepare data dengan proper data types untuk training\"\"\"\n",
    "    print(\"🔧 Preparing data for training...\")\n",
    "    \n",
    "    # Ensure we have a clean copy\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Get feature columns (exclude target)\n",
    "    feature_columns = [col for col in df_clean.columns if col != 'risk_encoded']\n",
    "    \n",
    "    print(f\"📊 Available features: {len(feature_columns)}\")\n",
    "    print(f\"Features: {feature_columns}\")\n",
    "    \n",
    "    if len(feature_columns) == 0:\n",
    "        raise ValueError(\"No feature columns found in dataset\")\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df_clean[feature_columns].copy()\n",
    "    y = df_clean['risk_encoded'].copy()\n",
    "    \n",
    "    # ✅ CRITICAL: Ensure all data is numeric and proper dtype\n",
    "    print(\"🔧 Converting to proper numeric types...\")\n",
    "    \n",
    "    # Convert X to float32 (consistent dtype)\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0).astype(np.float32)\n",
    "    \n",
    "    # Convert y to int32\n",
    "    y = pd.to_numeric(y, errors='coerce').fillna(0).astype(np.int32)\n",
    "    \n",
    "    # Verify data types\n",
    "    print(f\"✅ X dtypes: {X.dtypes.unique()}\")\n",
    "    print(f\"✅ y dtype: {y.dtype}\")\n",
    "    print(f\"✅ Data prepared: X shape={X.shape}, y shape={y.shape}\")\n",
    "    \n",
    "    # Final check - ensure no object dtypes remain\n",
    "    object_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    if object_cols:\n",
    "        raise ValueError(f\"Object columns still present: {object_cols}\")\n",
    "    \n",
    "    return X, y, feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eba23cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_fixed(X_train, X_test, y_train, y_test, feature_columns):\n",
    "    \"\"\"Train multiple models dengan improved error handling\"\"\"\n",
    "    print(\"🤖 Training multiple models with fixed serialization...\")\n",
    "    \n",
    "    # ✅ Ensure data is in correct format\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_train = y_train.astype(np.int32)\n",
    "    y_test = y_test.astype(np.int32)\n",
    "    \n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=50,  # Reduced for stability\n",
    "            random_state=42,\n",
    "            max_depth=8,\n",
    "            min_samples_split=5,\n",
    "            n_jobs=1  # ✅ Single job to avoid serialization issues\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=50,\n",
    "            random_state=42,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            eval_metric='logloss',\n",
    "            n_jobs=1  # ✅ Single job\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMClassifier(\n",
    "            n_estimators=50,\n",
    "            random_state=42,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            verbose=-1,\n",
    "            force_col_wise=True,\n",
    "            n_jobs=1  # ✅ Single job\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    # ✅ Use simple cross-validation without parallelization\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Reduced folds\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        try:\n",
    "            # ✅ Manual cross-validation to avoid serialization issues\n",
    "            cv_scores = []\n",
    "            for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "                X_train_fold = X_train.iloc[train_idx]\n",
    "                X_val_fold = X_train.iloc[val_idx]\n",
    "                y_train_fold = y_train.iloc[train_idx]\n",
    "                y_val_fold = y_train.iloc[val_idx]\n",
    "                \n",
    "                # Clone model for each fold\n",
    "                from sklearn.base import clone\n",
    "                model_clone = clone(model)\n",
    "                model_clone.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                y_pred_fold = model_clone.predict(X_val_fold)\n",
    "                score = accuracy_score(y_val_fold, y_pred_fold)\n",
    "                cv_scores.append(score)\n",
    "            \n",
    "            cv_scores = np.array(cv_scores)\n",
    "            \n",
    "            # Train on full training set\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Test predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'accuracy': float(test_accuracy),  # ✅ Ensure serializable\n",
    "                'f1_weighted': float(f1_weighted),\n",
    "                'f1_macro': float(f1_macro),\n",
    "                'cv_mean': float(cv_scores.mean()),\n",
    "                'cv_std': float(cv_scores.std()),\n",
    "                'cv_scores': cv_scores.tolist()\n",
    "            }\n",
    "            \n",
    "            trained_models[name] = model\n",
    "            \n",
    "            print(f\"✅ {name} - CV: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f}), Test: {test_accuracy:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {name} training failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            results[name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'f1_weighted': 0.0,\n",
    "                'f1_macro': 0.0,\n",
    "                'cv_mean': 0.0,\n",
    "                'cv_std': 0.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # Filter out failed models\n",
    "    valid_results = {name: result for name, result in results.items() if 'error' not in result}\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"⚠️ All models failed, creating dummy model...\")\n",
    "        # Create a simple dummy model that always works\n",
    "        from sklearn.dummy import DummyClassifier\n",
    "        dummy_model = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "        dummy_model.fit(X_train, y_train)\n",
    "        y_pred_dummy = dummy_model.predict(X_test)\n",
    "        \n",
    "        dummy_accuracy = accuracy_score(y_test, y_pred_dummy)\n",
    "        \n",
    "        results['DummyClassifier'] = {\n",
    "            'accuracy': float(dummy_accuracy),\n",
    "            'f1_weighted': float(f1_score(y_test, y_pred_dummy, average='weighted')),\n",
    "            'f1_macro': float(f1_score(y_test, y_pred_dummy, average='macro')),\n",
    "            'cv_mean': float(dummy_accuracy),\n",
    "            'cv_std': 0.0,\n",
    "            'cv_scores': [dummy_accuracy] * 3\n",
    "        }\n",
    "        \n",
    "        trained_models['DummyClassifier'] = dummy_model\n",
    "        valid_results = {'DummyClassifier': results['DummyClassifier']}\n",
    "        print(f\"✅ DummyClassifier - Accuracy: {dummy_accuracy:.4f}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(valid_results.keys(), key=lambda x: valid_results[x]['cv_mean'])\n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_accuracy = valid_results[best_model_name]['accuracy']\n",
    "    \n",
    "    # Get predictions from best model\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    \n",
    "    print(f\"🏆 Best model: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
    "    \n",
    "    return best_model, best_model_name, results, y_pred_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "95aca506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_importance_plot(model, feature_columns, model_name):\n",
    "    \"\"\"Create dan save feature importance plot\"\"\"\n",
    "    print(\"📊 Creating feature importance plot...\")\n",
    "    try:\n",
    "        # Extract feature importance dari model\n",
    "        if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "            importances = model.named_steps['classifier'].feature_importances_\n",
    "        elif hasattr(model.named_steps['classifier'], 'coef_'):\n",
    "            importances = np.abs(model.named_steps['classifier'].coef_[0])\n",
    "        else:\n",
    "            print(\"⚠️ Model doesn't have feature importance attribute\")\n",
    "            return\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Sort features by importance\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot\n",
    "        plt.bar(range(len(importances)), importances[indices])\n",
    "        plt.title(f'Feature Importance - {model_name}', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Features', fontsize=12)\n",
    "        plt.ylabel('Importance Score', fontsize=12)\n",
    "        \n",
    "        # Add feature names on x-axis\n",
    "        feature_names = [feature_columns[i] for i in indices]\n",
    "        plt.xticks(range(len(importances)), feature_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(importances[indices]):\n",
    "            plt.text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = \"results/feature_importance.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✅ Feature importance plot saved to {plot_path}\")\n",
    "        return plot_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error creating feature importance plot: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_model_comparison_plot(results, best_model_name):\n",
    "    \"\"\"Create dan save model comparison plot\"\"\"\n",
    "    print(\"📊 Creating model comparison plot...\")\n",
    "    try:\n",
    "        # Filter out models with errors\n",
    "        valid_results = {name: result for name, result in results.items() \n",
    "                        if 'error' not in result}\n",
    "        \n",
    "        if not valid_results:\n",
    "            print(\"⚠️ No valid results for comparison plot\")\n",
    "            return None\n",
    "        \n",
    "        models = list(valid_results.keys())\n",
    "        accuracies = [valid_results[model]['accuracy'] for model in models]\n",
    "        f1_scores = [valid_results[model]['f1_weighted'] for model in models]\n",
    "        cv_means = [valid_results[model]['cv_mean'] for model in models]\n",
    "        cv_stds = [valid_results[model]['cv_std'] for model in models]\n",
    "        \n",
    "        # Create subplot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Accuracy comparison\n",
    "        colors = ['gold' if model == best_model_name else 'lightblue' for model in models]\n",
    "        bars1 = ax1.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (model, bar) in enumerate(zip(models, bars1)):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                    f'{accuracies[i]:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax1.set_ylabel('Test Accuracy', fontsize=12)\n",
    "        ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Cross-validation scores with error bars\n",
    "        bars2 = ax2.bar(models, cv_means, yerr=cv_stds, capsize=5, \n",
    "                       color=colors, edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (model, bar) in enumerate(zip(models, bars2)):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + cv_stds[i] + 0.005,\n",
    "                    f'{cv_means[i]:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax2.set_ylabel('Cross-Validation Accuracy', fontsize=12)\n",
    "        ax2.set_title('Cross-Validation Performance\\n(Error bars show ±1 std dev)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = \"results/model_comparison.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✅ Model comparison plot saved to {plot_path}\")\n",
    "        return plot_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error creating model comparison plot: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_model_results_plot(y_test, y_pred, model_name, results):\n",
    "    \"\"\"Create dan save model results plot (confusion matrix + metrics)\"\"\"\n",
    "    print(\"📊 Creating model results plot...\")\n",
    "    try:\n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Create subplot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Confusion Matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "        ax1.set_title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "        ax1.set_ylabel('True Label', fontsize=12)\n",
    "        \n",
    "        # Plot 2: Performance Metrics Bar Chart\n",
    "        metrics = ['Accuracy', 'F1 Weighted', 'F1 Macro', 'CV Mean']\n",
    "        values = [\n",
    "            results['accuracy'],\n",
    "            results['f1_weighted'], \n",
    "            results['f1_macro'],\n",
    "            results['cv_mean']\n",
    "        ]\n",
    "        \n",
    "        bars = ax2.bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'],\n",
    "                      edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax2.set_ylabel('Score', fontsize=12)\n",
    "        ax2.set_title(f'Performance Metrics - {model_name}', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = \"results/model_results.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✅ Model results plot saved to {plot_path}\")\n",
    "        return plot_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error creating model results plot: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_all_visualizations(best_model, best_model_name, results, feature_columns, y_test, y_pred):\n",
    "    \"\"\"Save semua visualizations\"\"\"\n",
    "    print(\"🎨 Creating and saving all visualizations...\")\n",
    "    \n",
    "    visualization_paths = {}\n",
    "    \n",
    "    # 1. Feature Importance Plot\n",
    "    feature_plot_path = create_feature_importance_plot(best_model, feature_columns, best_model_name)\n",
    "    if feature_plot_path:\n",
    "        visualization_paths['feature_importance'] = feature_plot_path\n",
    "    \n",
    "    # 2. Model Comparison Plot\n",
    "    comparison_plot_path = create_model_comparison_plot(results, best_model_name)\n",
    "    if comparison_plot_path:\n",
    "        visualization_paths['model_comparison'] = comparison_plot_path\n",
    "    \n",
    "    # 3. Model Results Plot\n",
    "    results_plot_path = create_model_results_plot(y_test, y_pred, best_model_name, results[best_model_name])\n",
    "    if results_plot_path:\n",
    "        visualization_paths['model_results'] = results_plot_path\n",
    "    \n",
    "    return visualization_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "845dc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_metadata(best_model, best_model_name, results, feature_columns, encoders):\n",
    "    \"\"\"Save model dan metadata dengan proper serialization\"\"\"\n",
    "    print(\"💾 Saving model and metadata...\")\n",
    "    \n",
    "    try:\n",
    "        # Save model menggunakan skops\n",
    "        model_path = \"model/mental_health_pipeline.skops\"\n",
    "        sio.dump(best_model, model_path)\n",
    "        print(f\"✅ Model saved: {model_path}\")\n",
    "        \n",
    "        # ✅ Fix: Convert encoders untuk JSON serialization\n",
    "        encoders_serializable = {}\n",
    "        for key, encoder in encoders.items():\n",
    "            if hasattr(encoder, 'classes_'):\n",
    "                encoders_serializable[key] = {\n",
    "                    'classes_': encoder.classes_.tolist(),\n",
    "                    'type': 'LabelEncoder'\n",
    "                }\n",
    "            else:\n",
    "                encoders_serializable[key] = str(encoder)\n",
    "        \n",
    "        # Save encoders menggunakan pickle (bukan JSON)\n",
    "        encoders_path = \"model/encoders.pkl\"\n",
    "        with open(encoders_path, \"wb\") as f:\n",
    "            pickle.dump(encoders, f)\n",
    "        print(f\"✅ Encoders saved: {encoders_path}\")\n",
    "        \n",
    "        # Save feature columns\n",
    "        feature_path = \"model/feature_columns.pkl\"\n",
    "        with open(feature_path, \"wb\") as f:\n",
    "            pickle.dump(feature_columns, f)\n",
    "        print(f\"✅ Feature columns saved: {feature_path}\")\n",
    "        \n",
    "        # ✅ Create JSON-serializable metadata\n",
    "        metadata = {\n",
    "            'best_model_name': best_model_name,\n",
    "            'feature_columns': feature_columns,\n",
    "            'test_accuracy': float(results[best_model_name]['accuracy']),\n",
    "            'test_f1': float(results[best_model_name]['f1_weighted']),\n",
    "            'cv_mean': float(results[best_model_name]['cv_mean']),\n",
    "            'cv_std': float(results[best_model_name]['cv_std']),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'encoders_info': encoders_serializable  # JSON-serializable version\n",
    "        }\n",
    "        \n",
    "        # Save metadata sebagai JSON\n",
    "        metadata_path = \"model/model_metadata.json\"\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"✅ Metadata saved: {metadata_path}\")\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving model artifacts: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "18587197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_summary(metadata, data_source):\n",
    "    \"\"\"Create comprehensive training summary untuk monitoring\"\"\"\n",
    "    print(\"📋 Creating training summary...\")\n",
    "    \n",
    "    training_summary = {\n",
    "        \"training_completed\": True,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"data_source\": data_source,\n",
    "        \"best_model\": metadata['model_name'],\n",
    "        \"best_accuracy\": metadata['best_accuracy'],\n",
    "        \"models_trained\": list(metadata['all_models_performance'].keys()),\n",
    "        \"feature_count\": len(metadata['feature_columns']),\n",
    "        \"status\": \"success\",\n",
    "        \"model_path\": metadata['model_path'],\n",
    "        \"metadata_path\": \"models/model_metadata.json\"\n",
    "    }\n",
    "    \n",
    "    # Save training summary\n",
    "    with open(\"monitoring/training_summary.json\", \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "    \n",
    "    # Save untuk CML report\n",
    "    with open(\"results/training_results.json\", \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "    \n",
    "    print(\"✅ Training summary created for monitoring integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ac3ec23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting ML Training Pipeline with Evidently Integration\n",
      "============================================================\n",
      "✅ Directories setup completed\n",
      "✅ Dataset loaded from: data/mental_health_lite.csv\n",
      "📊 Original data shape: (1001, 14)\n",
      "📊 Original data types:\n",
      "age                         int64\n",
      "gender                     object\n",
      "employment_status          object\n",
      "work_environment           object\n",
      "mental_health_history      object\n",
      "seeks_treatment            object\n",
      "stress_level                int64\n",
      "sleep_hours               float64\n",
      "physical_activity_days      int64\n",
      "depression_score            int64\n",
      "anxiety_score               int64\n",
      "social_support_score        int64\n",
      "productivity_score        float64\n",
      "mental_health_risk         object\n",
      "dtype: object\n",
      "🔧 Encoding gender...\n",
      "✅ gender encoded: ['Female' 'Male' 'Non-binary' 'Prefer not to say'] -> [0, 1, 2, 3]\n",
      "🔧 Encoding employment_status...\n",
      "✅ employment_status encoded: ['Employed' 'Self-employed' 'Student' 'Unemployed'] -> [0, 1, 2, 3]\n",
      "🔧 Encoding work_environment...\n",
      "✅ work_environment encoded: ['Hybrid' 'On-site' 'Remote'] -> [0, 1, 2]\n",
      "🔧 Encoding mental_health_history...\n",
      "✅ mental_health_history encoded: ['No' 'Yes'] -> [0, 1]\n",
      "🔧 Encoding seeks_treatment...\n",
      "✅ seeks_treatment encoded: ['No' 'Yes'] -> [0, 1]\n",
      "✅ Target encoded: ['High' 'Low' 'Medium'] -> [0, 1, 2]\n",
      "📊 Cleaned data shape: (1001, 14)\n",
      "📊 Cleaned data types:\n",
      "age                                int64\n",
      "stress_level                       int64\n",
      "sleep_hours                      float64\n",
      "physical_activity_days             int64\n",
      "depression_score                   int64\n",
      "anxiety_score                      int64\n",
      "social_support_score               int64\n",
      "productivity_score               float64\n",
      "gender_encoded                     int32\n",
      "employment_status_encoded          int32\n",
      "work_environment_encoded           int32\n",
      "mental_health_history_encoded      int32\n",
      "seeks_treatment_encoded            int32\n",
      "risk_encoded                       int32\n",
      "dtype: object\n",
      "✅ Reference data saved for future monitoring\n",
      "🔍 Starting simplified Evidently monitoring...\n",
      "✅ Monitoring directory created/verified\n",
      "✅ Data loaded from: data/mental_health_lite.csv (1001 rows)\n",
      "📊 Data split: Total=1001, Reference=700, Current=301\n",
      "✅ Summary saved to: monitoring/evidently_summary.json\n",
      "✅ Summary file verified: 272 bytes\n",
      "✅ Evidently monitoring completed successfully\n",
      "✅ Evidently monitoring completed\n",
      "🔧 Preparing data for training...\n",
      "📊 Available features: 13\n",
      "Features: ['age', 'stress_level', 'sleep_hours', 'physical_activity_days', 'depression_score', 'anxiety_score', 'social_support_score', 'productivity_score', 'gender_encoded', 'employment_status_encoded', 'work_environment_encoded', 'mental_health_history_encoded', 'seeks_treatment_encoded']\n",
      "🔧 Converting to proper numeric types...\n",
      "✅ X dtypes: [dtype('float32')]\n",
      "✅ y dtype: int32\n",
      "✅ Data prepared: X shape=(1001, 13), y shape=(1001,)\n",
      "✅ Using target column: risk_encoded\n",
      "📊 Target distribution:\n",
      "risk_encoded\n",
      "2    589\n",
      "0    232\n",
      "1    180\n",
      "Name: count, dtype: int64\n",
      "📊 Training data shape: X=(1001, 13), y=(1001,)\n",
      "📊 Train: (700, 13), Test: (301, 13)\n",
      "🤖 Training multiple models with fixed serialization...\n",
      "Training RandomForest...\n",
      "✅ RandomForest - CV: 0.9114 (+/- 0.0204), Test: 0.9236\n",
      "Training XGBoost...\n",
      "✅ XGBoost - CV: 0.9486 (+/- 0.0072), Test: 0.9635\n",
      "Training LightGBM...\n",
      "✅ LightGBM - CV: 0.9586 (+/- 0.0147), Test: 0.9668\n",
      "🏆 Best model: LightGBM (Accuracy: 0.9668)\n",
      "💾 Saving model and metadata...\n",
      "✅ Model saved: model/mental_health_pipeline.skops\n",
      "✅ Encoders saved: model/encoders.pkl\n",
      "✅ Feature columns saved: model/feature_columns.pkl\n",
      "✅ Metadata saved: model/model_metadata.json\n",
      "🎨 Creating and saving all visualizations...\n",
      "📊 Creating feature importance plot...\n",
      "⚠️ Error creating feature importance plot: 'LGBMClassifier' object has no attribute 'named_steps'\n",
      "📊 Creating model comparison plot...\n",
      "✅ Model comparison plot saved to results/model_comparison.png\n",
      "📊 Creating model results plot...\n",
      "✅ Model results plot saved to results/model_results.png\n",
      "============================================================\n",
      "🎉 Training pipeline completed successfully!\n",
      "🏆 Best model: LightGBM\n",
      "📊 Best accuracy: 0.9668\n",
      "🎨 Visualizations created:\n",
      "   - model_comparison: results/model_comparison.png\n",
      "   - model_results: results/model_results.png\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "     # ✅ Import fungsi monitoring dari scripts\n",
    "    try:\n",
    "        from scripts.evidently_monitoring import run_evidently_monitoring\n",
    "        EVIDENTLY_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Evidently monitoring script not found\")\n",
    "        EVIDENTLY_AVAILABLE = False\n",
    "    \n",
    "    print(\"🚀 Starting ML Training Pipeline with Evidently Integration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ✅ Setup directories\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        os.makedirs(\"explanations\", exist_ok=True)\n",
    "        os.makedirs(\"monitoring/evidently_reports\", exist_ok=True)\n",
    "        print(\"✅ Directories setup completed\")\n",
    "        \n",
    "        # ✅ Load and prepare data (menggunakan fungsi yang sudah ada)\n",
    "        df, encoders = load_and_prepare_data()\n",
    "        data_source = \"mental_health_lite\"\n",
    "        \n",
    "        # ✅ Skip Evidently monitoring jika ada masalah (dengan proper check)\n",
    "        if EVIDENTLY_AVAILABLE:\n",
    "            try:\n",
    "                numeric_df = df.select_dtypes(include=['number'])\n",
    "                if len(numeric_df.columns) > 0:\n",
    "                    run_evidently_monitoring()\n",
    "                    print(\"✅ Evidently monitoring completed\")\n",
    "                else:\n",
    "                    print(\"⚠️ No numeric columns for Evidently monitoring\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Evidently monitoring skipped: {e}\")\n",
    "        else:\n",
    "            print(\"⚠️ Evidently monitoring not available\")\n",
    "        \n",
    "        # ✅ Prepare data for training (menggunakan fungsi yang sudah ada)\n",
    "        X, y, feature_columns = prepare_data_for_training(df, encoders)\n",
    "        target_column = 'risk_encoded'  # Target sudah ditentukan dalam prepare_data_for_training\n",
    "        \n",
    "        print(f\"✅ Using target column: {target_column}\")\n",
    "        print(f\"📊 Target distribution:\\n{y.value_counts()}\")\n",
    "        print(f\"📊 Training data shape: X={X.shape}, y={y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42, stratify=y  # Ubah ke 0.3 sesuai implementasi sebelumnya\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        # ✅ Train models (menggunakan fungsi yang sudah ada)\n",
    "        best_model, best_model_name, results, y_pred = train_models_fixed(\n",
    "            X_train, X_test, y_train, y_test, feature_columns\n",
    "        )\n",
    "        \n",
    "        # ✅ Save model and metadata (menggunakan fungsi yang sudah ada)\n",
    "        metadata = save_model_and_metadata(\n",
    "            best_model, best_model_name, results, feature_columns, encoders\n",
    "        )\n",
    "        \n",
    "        # ✅ Create and save all visualizations (menggunakan fungsi yang sudah ada)\n",
    "        visualization_paths = save_all_visualizations(\n",
    "            best_model, best_model_name, results, feature_columns, y_test, y_pred\n",
    "        )\n",
    "        \n",
    "        # Add visualization paths to metadata\n",
    "        metadata['visualization_paths'] = visualization_paths\n",
    "        \n",
    "        # ✅ Update metadata file with visualization info (fix path)\n",
    "        metadata_path = \"model/model_metadata.json\"  # Ubah dari \"models/\" ke \"model/\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        # ✅ Create training summary (simplified)\n",
    "        training_summary = {\n",
    "            \"training_completed\": True,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"best_model\": best_model_name,\n",
    "            \"best_accuracy\": metadata.get('test_accuracy', 0.0),\n",
    "            \"data_source\": data_source,\n",
    "            \"feature_count\": len(feature_columns),\n",
    "            \"training_samples\": len(X_train),\n",
    "            \"test_samples\": len(X_test),\n",
    "            \"visualization_paths\": visualization_paths,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/training_summary.json\", \"w\") as f:\n",
    "            json.dump(training_summary, f, indent=2)\n",
    "        \n",
    "        with open(\"results/training_results.json\", \"w\") as f:\n",
    "            json.dump(training_summary, f, indent=2)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"🎉 Training pipeline completed successfully!\")\n",
    "        print(f\"🏆 Best model: {best_model_name}\")\n",
    "        print(f\"📊 Best accuracy: {metadata.get('test_accuracy', 0.0):.4f}\")\n",
    "        print(\"🎨 Visualizations created:\")\n",
    "        for viz_type, path in visualization_paths.items():\n",
    "            print(f\"   - {viz_type}: {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # ✅ Create error summary dengan proper directory creation\n",
    "        os.makedirs(\"monitoring\", exist_ok=True)\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        \n",
    "        error_summary = {\n",
    "            \"training_completed\": False,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"failed\"\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/training_summary.json\", \"w\") as f:\n",
    "            json.dump(error_summary, f, indent=2)\n",
    "        \n",
    "        with open(\"results/training_results.json\", \"w\") as f:\n",
    "            json.dump(error_summary, f, indent=2)\n",
    "        \n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
