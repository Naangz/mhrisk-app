{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25301e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import skops.io as sio\n",
    "import shap\n",
    "import pickle\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scripts.evidently_monitoring import EvidentlyMonito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5798b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load dan prepare data dengan Evidently monitoring saja\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    data_files = ['data/mental_health_lite.csv', 'data/mental_health_life_cut.csv']\n",
    "    df = None\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"‚úÖ Dataset loaded from: {file_path}\")\n",
    "            break\n",
    "    \n",
    "    if df is None:\n",
    "        raise FileNotFoundError(\"No dataset found\")\n",
    "    \n",
    "    # Evidently monitoring (menggantikan WhyLogs)\n",
    "    try:\n",
    "        monitor = EvidentlyMonitor()\n",
    "        \n",
    "        # Check if reference data exists\n",
    "        reference_path = \"monitoring/reference_data.csv\"\n",
    "        if os.path.exists(reference_path):\n",
    "            reference_data = pd.read_csv(reference_path)\n",
    "            monitor.set_reference_data(reference_data)\n",
    "            \n",
    "            # Run comprehensive monitoring\n",
    "            results = monitor.monitor_mental_health_data(df)\n",
    "            \n",
    "            if results[\"drift_results\"][\"drift_detected\"]:\n",
    "                print(f\"üö® Data drift detected in {results['drift_results']['total_drifted']} columns\")\n",
    "                \n",
    "                # Log drift details\n",
    "                for col_info in results['drift_results']['drifted_columns']:\n",
    "                    print(f\"  - {col_info['column']}: drift score {col_info['drift_score']:.3f}\")\n",
    "            else:\n",
    "                print(\"‚úÖ No significant data drift detected\")\n",
    "        else:\n",
    "            # Save current data as reference\n",
    "            df.to_csv(reference_path, index=False)\n",
    "            print(\"‚úÖ Reference data saved for future monitoring\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Evidently monitoring error: {e}\")\n",
    "        print(\"Continuing without monitoring...\")\n",
    "    \n",
    "    # Encode categorical variables (tanpa WhyLogs profiling)\n",
    "    encoders = {}\n",
    "    categorical_cols = ['gender', 'employment_status', 'work_environment', \n",
    "                       'mental_health_history', 'seeks_treatment']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "            encoders[col] = le\n",
    "            print(f\"‚úÖ Encoded {col}: {len(le.classes_)} classes\")\n",
    "    \n",
    "    # Encode target variable\n",
    "    if 'mental_health_risk' in df.columns:\n",
    "        le_risk = LabelEncoder()\n",
    "        df['risk_encoded'] = le_risk.fit_transform(df['mental_health_risk'])\n",
    "        encoders['risk'] = le_risk\n",
    "        print(f\"‚úÖ Encoded target: {le_risk.classes_}\")\n",
    "    \n",
    "    # Save encoders\n",
    "    os.makedirs(\"model\", exist_ok=True)\n",
    "    with open(\"model/encoders.pkl\", \"wb\") as f:\n",
    "        pickle.dump(encoders, f)\n",
    "    \n",
    "    return df, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6b4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_pipelines():\n",
    "    \"\"\"Create multiple ML pipelines\"\"\"\n",
    "\n",
    "    models = {\n",
    "        \"RandomForest\": Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\n",
    "                    \"classifier\",\n",
    "                    RandomForestClassifier(\n",
    "                        n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        \"XGBoost\": Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\n",
    "                    \"classifier\",\n",
    "                    xgb.XGBClassifier(\n",
    "                        objective=\"multi:softprob\",\n",
    "                        learning_rate=0.1,\n",
    "                        max_depth=6,\n",
    "                        n_estimators=100,\n",
    "                        colsample_bytree=0.8,\n",
    "                        subsample=0.8,\n",
    "                        random_state=42,\n",
    "                        eval_metric=\"mlogloss\",\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        \"LightGBM\": Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\n",
    "                    \"classifier\",\n",
    "                    lgb.LGBMClassifier(\n",
    "                        objective=\"multiclass\",\n",
    "                        num_class=3,\n",
    "                        learning_rate=0.1,\n",
    "                        max_depth=6,\n",
    "                        n_estimators=100,\n",
    "                        feature_fraction=0.8,\n",
    "                        bagging_fraction=0.8,\n",
    "                        bagging_freq=5,\n",
    "                        random_state=42,\n",
    "                        verbosity=-1,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97b72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_select_best_model():\n",
    "    \"\"\"Training multiple models dan pilih yang terbaik\"\"\"\n",
    "\n",
    "    print(\"üìä Loading and preparing data...\")\n",
    "    df, encoders = load_and_prepare_data()\n",
    "\n",
    "    # Identify numeric columns for features\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Remove target columns from features\n",
    "    feature_columns = [col for col in numeric_cols if not col.endswith(\"_encoded\") or col != \"risk_encoded\"]\n",
    "\n",
    "    # Add encoded categorical features\n",
    "    encoded_cols = [col for col in df.columns if col.endswith(\"_encoded\") and col != \"risk_encoded\"]\n",
    "    feature_columns.extend(encoded_cols)\n",
    "\n",
    "    # Filter existing columns\n",
    "    available_features = [col for col in feature_columns if col in df.columns]\n",
    "    print(f\"üìã Available features: {len(available_features)}\")\n",
    "    print(f\"Features: {available_features}\")\n",
    "\n",
    "    if len(available_features) == 0:\n",
    "        raise ValueError(\"No feature columns found in dataset\")\n",
    "\n",
    "    X = df[available_features]\n",
    "    y = df[\"risk_encoded\"]\n",
    "\n",
    "    print(f\"üìä Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Create model pipelines\n",
    "    models = create_model_pipelines()\n",
    "\n",
    "    # Train and evaluate models\n",
    "    model_scores = {}\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"üîÑ Training and evaluating models...\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüöÄ Training {name}...\")\n",
    "\n",
    "        try:\n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "            mean_score = cv_scores.mean()\n",
    "            std_score = cv_scores.std()\n",
    "\n",
    "            model_scores[name] = {\n",
    "                \"cv_scores\": cv_scores.tolist(),\n",
    "                \"mean_accuracy\": mean_score,\n",
    "                \"std_accuracy\": std_score,\n",
    "                \"model\": model,\n",
    "            }\n",
    "\n",
    "            print(f\"‚úÖ {name} - CV Accuracy: {mean_score:.4f} (+/- {std_score * 2:.4f})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {name} failed: {e}\")\n",
    "            model_scores[name] = {\n",
    "                \"cv_scores\": [0.0],\n",
    "                \"mean_accuracy\": 0.0,\n",
    "                \"std_accuracy\": 0.0,\n",
    "                \"model\": model,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "    # Select best model\n",
    "    valid_models = {k: v for k, v in model_scores.items() if \"error\" not in v}\n",
    "\n",
    "    if not valid_models:\n",
    "        raise ValueError(\"No models trained successfully\")\n",
    "\n",
    "    best_model_name = max(valid_models.keys(), key=lambda x: valid_models[x][\"mean_accuracy\"])\n",
    "    best_model = valid_models[best_model_name][\"model\"]\n",
    "    best_score = valid_models[best_model_name][\"mean_accuracy\"]\n",
    "\n",
    "    print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "    print(f\"üìä CV Accuracy: {best_score:.4f}\")\n",
    "\n",
    "    # Train best model\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Final evaluation\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\nüìä Final Test Results:\")\n",
    "    print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"   Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    # Create results directories\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(\"explanations\", exist_ok=True)\n",
    "\n",
    "    # Save results\n",
    "    comparison_results = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"best_model\": best_model_name,\n",
    "        \"model_scores\": {\n",
    "            name: {\"mean_accuracy\": scores[\"mean_accuracy\"], \"std_accuracy\": scores[\"std_accuracy\"]}\n",
    "            for name, scores in valid_models.items()\n",
    "        },\n",
    "        \"final_test_accuracy\": test_accuracy,\n",
    "        \"final_test_f1\": test_f1,\n",
    "    }\n",
    "\n",
    "    with open(\"results/model_comparison.json\", \"w\") as f:\n",
    "        json.dump(comparison_results, f, indent=2)\n",
    "\n",
    "    # Save metrics\n",
    "    with open(\"results/metrics.txt\", \"w\") as f:\n",
    "        f.write(f\"Best Model: {best_model_name}\\n\")\n",
    "        f.write(f\"CV Accuracy: {best_score:.4f}\\n\")\n",
    "        f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Test F1 Score: {test_f1:.4f}\\n\\n\")\n",
    "        f.write(\"Model Comparison:\\n\")\n",
    "        for name, scores in valid_models.items():\n",
    "            f.write(f\"{name}: {scores['mean_accuracy']:.4f} (+/- {scores['std_accuracy']*2:.4f})\\n\")\n",
    "        f.write(f\"\\nClassification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "    # Save model\n",
    "    sio.dump(best_model, \"model/mental_health_pipeline.skops\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"best_model_name\": best_model_name,\n",
    "        \"feature_columns\": available_features,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    with open(\"model/model_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    with open(\"model/feature_columns.pkl\", \"wb\") as f:\n",
    "        pickle.dump(available_features, f)\n",
    "\n",
    "    print(\"‚úÖ Model training completed successfully!\")\n",
    "\n",
    "    return best_model, best_model_name, test_accuracy, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517689c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading and preparing data...\n",
      "‚úÖ Dataset loaded from: data/mental_health_lite.csv\n",
      "‚ùì What kind of session do you want to use?\n",
      " ‚§∑ 1. WhyLabs. Use an api key to upload to WhyLabs.\n",
      " ‚§∑ 2. Local. Don't upload data anywhere.\n",
      "\n",
      "‚ùì Do you want to automatically upload profiles in why.log()?\n",
      " ‚§∑ 1. No. Use an explicit WhyLabsWriter to manage uploads to WhyLabs\n",
      " ‚§∑ 2. Yes. Calling why.log() will automatically upload the results to WhyLabs\n",
      "\n",
      "Initializing session with config C:\\Users\\User\\AppData\\Local\\whylogs\\whylogs\\config.ini\n",
      "\n",
      "‚úÖ Using session type: WHYLABS_ANONYMOUS\n",
      " ‚§∑ session id: <will be generated before upload>\n",
      "‚úÖ WhyLogs session initialized (LOCAL)\n",
      "\n",
      "‚úÖ Aggregated 1001 rows into profile \n",
      "\n",
      "‚ùå Failed to upload profile: HTTPSConnectionPool(host='api.whylabsapp.com', port=443): Max retries exceeded with url: /v0/sessions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DB78B60410>: Failed to resolve 'api.whylabsapp.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "‚úÖ WhyLogs profile saved successfully\n",
      "‚úÖ Encoded gender: 4 classes\n",
      "‚úÖ Encoded employment_status: 4 classes\n",
      "‚úÖ Encoded work_environment: 3 classes\n",
      "‚úÖ Encoded mental_health_history: 2 classes\n",
      "‚úÖ Encoded seeks_treatment: 2 classes\n",
      "‚úÖ Encoded target (mental_health_risk): ['High' 'Low' 'Medium']\n",
      "üìã Available features: 18\n",
      "Features: ['age', 'stress_level', 'sleep_hours', 'physical_activity_days', 'depression_score', 'anxiety_score', 'social_support_score', 'productivity_score', 'gender_encoded', 'employment_status_encoded', 'work_environment_encoded', 'mental_health_history_encoded', 'seeks_treatment_encoded', 'gender_encoded', 'employment_status_encoded', 'work_environment_encoded', 'mental_health_history_encoded', 'seeks_treatment_encoded']\n",
      "üìä Dataset shape: X=(1001, 18), y=(1001,)\n",
      "üîÑ Training and evaluating models...\n",
      "\n",
      "üöÄ Training RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "        (self._dns_host, self.port),\n",
      "    ...<2 lines>...\n",
      "        socket_options=self.socket_options,\n",
      "    )\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\util\\connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\socket.py\", line 975, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "               ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "        conn,\n",
      "    ...<10 lines>...\n",
      "        **response_kw,\n",
      "    )\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n",
      "    raise new_e\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "    ~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connection.py\", line 704, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "                       ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connection.py\", line 205, in _new_conn\n",
      "    raise NameResolutionError(self.host, self, e) from e\n",
      "urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x000001DB78B60410>: Failed to resolve 'api.whylabsapp.com' ([Errno 11001] getaddrinfo failed)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylogs\\api\\whylabs\\session\\notebook_logger.py\", line 90, in notebook_session_log\n",
      "    result = session.upload_batch_profile(result_set)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylogs\\api\\whylabs\\session\\session.py\", line 184, in upload_batch_profile\n",
      "    session_id = self._get_or_create_session_id()\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylogs\\api\\whylabs\\session\\session.py\", line 126, in _get_or_create_session_id\n",
      "    session_id = self._create_session_id()\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylogs\\api\\whylabs\\session\\session.py\", line 158, in _create_session_id\n",
      "    response: CreateSessionResponse = self._whylabs_session_api.value.create_session(  # type: ignore\n",
      "                                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "        CreateSessionRequest(user_guid)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylabs_client\\api\\sessions_api.py\", line 799, in create_session\n",
      "    return self.create_session_endpoint.call_with_http_info(**kwargs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylabs_client\\api_client.py\", line 849, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self.settings['endpoint_path'], self.settings['http_method'],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<13 lines>...\n",
      "        _host=_host,\n",
      "        ^^^^^^^^^^^^\n",
      "        collection_formats=params['collection_format'])\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylabs_client\\api_client.py\", line 410, in call_api\n",
      "    return self.__call_api(resource_path, method,\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                           path_params, query_params, header_params,\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "                           _preload_content, _request_timeout, _host,\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                           _check_type)\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylabs_client\\api_client.py\", line 197, in __call_api\n",
      "    response_data = self.request(\n",
      "        method, url, query_params=query_params, headers=header_params,\n",
      "        post_params=post_params, body=body,\n",
      "        _preload_content=_preload_content,\n",
      "        _request_timeout=_request_timeout)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylabs_client\\api_client.py\", line 456, in request\n",
      "    return self.rest_client.POST(url,\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "                                 query_params=query_params,\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "                                 _request_timeout=_request_timeout,\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                 body=body)\n",
      "                                 ^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylabs_client\\rest.py\", line 269, in POST\n",
      "    return self.request(\"POST\", url,\n",
      "           ~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "                        headers=headers,\n",
      "                        ^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "                        _request_timeout=_request_timeout,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        body=body)\n",
      "                        ^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\whylabs_client\\rest.py\", line 155, in request\n",
      "    r = self.pool_manager.request(\n",
      "        method, url,\n",
      "    ...<2 lines>...\n",
      "        timeout=timeout,\n",
      "        headers=headers)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\_request_methods.py\", line 143, in request\n",
      "    return self.request_encode_body(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        method, url, fields=fields, headers=headers, **urlopen_kw\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\_request_methods.py\", line 278, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\poolmanager.py\", line 443, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<13 lines>...\n",
      "        **response_kw,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<13 lines>...\n",
      "        **response_kw,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 871, in urlopen\n",
      "    return self.urlopen(\n",
      "           ~~~~~~~~~~~~^\n",
      "        method,\n",
      "        ^^^^^^^\n",
      "    ...<13 lines>...\n",
      "        **response_kw,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "    )\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.whylabsapp.com', port=443): Max retries exceeded with url: /v0/sessions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DB78B60410>: Failed to resolve 'api.whylabsapp.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_71912\\2066757514.py:30: DeprecationWarning: please use a Writer\n",
      "  profile_view.write(\"monitoring/whylogs_profiles/training_data_profile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RandomForest - CV Accuracy: 0.9243 (+/- 0.0508)\n",
      "\n",
      "üöÄ Training XGBoost...\n",
      "‚úÖ XGBoost - CV Accuracy: 0.9529 (+/- 0.0466)\n",
      "\n",
      "üöÄ Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\envtr1\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The LGBMClassifier or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LightGBM - CV Accuracy: 0.9514 (+/- 0.0567)\n",
      "\n",
      "üèÜ BEST MODEL: XGBoost\n",
      "üìä CV Accuracy: 0.9529\n",
      "\n",
      "üìä Final Test Results:\n",
      "   Test Accuracy: 0.9701\n",
      "   Test F1 Score: 0.9700\n",
      "‚úÖ Model training completed successfully!\n",
      "\n",
      "üéâ Training completed successfully!\n",
      "üèÜ Best model: XGBoost\n",
      "üìä Final accuracy: 0.9701\n",
      "üìä Final F1 score: 0.9700\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        best_model, best_model_name, accuracy, f1 = train_and_select_best_model()\n",
    "        print(f\"\\nüéâ Training completed successfully!\")\n",
    "        print(f\"üèÜ Best model: {best_model_name}\")\n",
    "        print(f\"üìä Final accuracy: {accuracy:.4f}\")\n",
    "        print(f\"üìä Final F1 score: {f1:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
